{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "term-statistics.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jessiecuixy/CS6120/blob/main/token_counts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yYCQrU0J5C5"
      },
      "source": [
        "# Empirical Regularities of Language\n",
        "\n",
        "In this first homework assignment, you will familiarize yourself with some empirical regularities of natural language, Shannon entropy and Zipf's Law.\n",
        "\n",
        "Read through this Jupyter notebook and fill in the parts marked with `TODO`. When you're ready to submit, print the notebook as a PDF and upload to Gradescope.\n",
        "\n",
        "## Shannon Entropy\n",
        "\n",
        "Shannon borrowed the concept of entropy from statistical physics to develop _information theory_, focused on encoding and compressing messages. A few years later, in 1950, he applied information theory to analyze human predictive abilityâ€”in other words, the entropy of the human language model. You can read the original article, [Prediction and Entropy of Printed English](https://languagelog.ldc.upenn.edu/myl/Shannon1950.pdf), for more details.\n",
        "\n",
        "Your first task is to collect data on how predictable different letters are in an English sentence, depending on how much context in a word or sentence you have.\n",
        "\n",
        "Go to the [Shannon game page](https://www.ccs.neu.edu/home/dasmith/courses/cs6120/shannon/) that we demonstrated in class. We already guessed part of Text 1, so work through Texts 2, 3, and 4."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Enter the arrays of numbers of guesses for Texts 2, 3, and 4 here.\n",
        "[4,4,2,9,3,1,1,1,25,1,3,20,1,1,1,1,3,1,4,3,16,14,2,1,1,1,1,16,1,5,11,1,22,3,1,8,1,10,26,1,7,1,2,1,1,1,1]\n",
        "[5,6,27,13,10,4,1,1,25,20,25,7,3,5,1,1,2,1,2,1,17,13,1,1,1,26,3,16,3,3,1,1,1,1,1,14,1,3,1,1,1,4,2,11,20,18,14,2,16,20,6,12,21,1,1,13,17,27,17,18,10,1,1,21,18,12,10]\n",
        "[19,1,17,7,1,17,22,3,1,1,1,1,1,1,1,22,1,8,1,1,1,1,1,1,12,4,3,2,1,1,16,1,6,4,6,16,2,1,1,14,5,1,21,2,22,16,25,2,2,1,1,1,1,13,1,1,1]"
      ],
      "metadata": {
        "id": "n6siYs-p0-_E"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rearrange the guess data into a two-dimensional array, relating number of characters of context (0, 1, 2, ...) to number of guesses required.\n",
        "\n",
        "In other words, you might look in cell (2, 1) and read \"2\" if the number of times it took one guess to get the right letter with two characters of context was 2."
      ],
      "metadata": {
        "id": "3P2UvFmj2Ep2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Create array of counts of guesses. Print out the array so we can see it.\n",
        "import numpy as np\n",
        "\n",
        "text2_guesses = [4,4,2,9,3,1,1,1,25,1,3,20,1,1,1,1,3,1,4,3,16,14,2,1,1,1,1,16,1,5,11,1,22,3,1,8,1,10,26,1,7,1,2,1,1,1,1]\n",
        "text3_guesses = [5,6,27,13,10,4,1,1,25,20,25,7,3,5,1,1,2,1,2,1,17,13,1,1,1,26,3,16,3,3,1,1,1,1,1,14,1,3,1,1,1,4,2,11,20,18,14,2,16,20,6,12,21,1,1,13,17,27,17,18,10,1,1,21,18,12,10]\n",
        "text4_guesses = [19,1,17,7,1,17,22,3,1,1,1,1,1,1,1,22,1,8,1,1,1,1,1,1,12,4,3,2,1,1,16,1,6,4,6,16,2,1,1,14,5,1,21,2,22,16,25,2,2,1,1,1,1,13,1,1,1]\n",
        "\n",
        "context_for_text = {\n",
        "    0: text2_guesses,\n",
        "    1: text3_guesses,\n",
        "    2: text4_guesses,\n",
        "}\n",
        "\n",
        "MAX_GUESS = 27\n",
        "max_N = max(context_for_text.keys())\n",
        "\n",
        "q_counts = np.zeros((MAX_GUESS + 1, max_N + 1), dtype=int)\n",
        "\n",
        "for N, guesses in context_for_text.items():\n",
        "    counts = np.bincount(guesses, minlength=MAX_GUESS + 1)\n",
        "    q_counts[:, N] = counts[:MAX_GUESS+1]\n",
        "\n",
        "print(q_counts[1:])\n",
        "#print(q_counts[1:].sum(axis=0))\n"
      ],
      "metadata": {
        "id": "deK9pKWC1Qp_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00eacafe-a5f1-4a51-86fb-99e95f399099"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[23 22 29]\n",
            " [ 3  4  5]\n",
            " [ 5  5  2]\n",
            " [ 3  2  2]\n",
            " [ 1  2  1]\n",
            " [ 0  2  2]\n",
            " [ 1  1  1]\n",
            " [ 1  0  1]\n",
            " [ 1  0  0]\n",
            " [ 1  3  0]\n",
            " [ 1  1  0]\n",
            " [ 0  2  1]\n",
            " [ 0  3  1]\n",
            " [ 1  2  1]\n",
            " [ 0  0  0]\n",
            " [ 2  2  3]\n",
            " [ 0  3  2]\n",
            " [ 0  3  0]\n",
            " [ 0  0  1]\n",
            " [ 1  3  0]\n",
            " [ 0  2  1]\n",
            " [ 1  0  3]\n",
            " [ 0  0  0]\n",
            " [ 0  0  0]\n",
            " [ 1  2  1]\n",
            " [ 1  1  0]\n",
            " [ 0  2  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can compute Shannon's upper and lower bounds on the entropy of your predictive distribution for English. The upper bound, as a function of the number of context characters $N$, is just the Shannon entropy of the distribution of numbers of guesses. In other words, it's the entropy of the original text as &ldquo;reduced&rdquo; by the human encoder to a sequence of numbers of guesses.\n",
        "\n",
        "$F_N = -\\sum_{i=1}^{27} q_i^N \\log_2 q_i^N$\n",
        "\n",
        "where $q_i^N$ is the number of times you took $i$ guesses with $N$ characters of context, i.e., one of the cells in the table you computed. The number of guesses ranges from 1 to 27 since we restrict ourselves to 26 letters plus space. In computing entropy, we define $0 \\log 0 = 0$."
      ],
      "metadata": {
        "id": "BR9BqdYO2eTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Compute the upper bound for each amount of context N and print it out.\n",
        "import numpy as np\n",
        "\n",
        "q_only = q_counts[1:]\n",
        "\n",
        "col_sums = q_only.sum(axis=0, keepdims=True)\n",
        "q_probs = q_only / col_sums\n",
        "\n",
        "entropy_upper = -np.sum(q_probs * np.log2(q_probs, where=(q_probs > 0)), axis=0)\n",
        "\n",
        "for N, H in enumerate(entropy_upper):\n",
        "    print(f\"{H:.4f} bits\")"
      ],
      "metadata": {
        "id": "4ubcFqVv4CjC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c278bc9-8fd9-43bd-e44a-098feddf0768"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8490 bits\n",
            "3.6856 bits\n",
            "2.8504 bits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shannon derived a lower bound on entropy from the guess data as\n",
        "\n",
        "$\\sum_{i=1}^{27} i(q_i^N - q_{i+1}^N) \\log_2 i$"
      ],
      "metadata": {
        "id": "NRULmLAM4I5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Compute thew lower bound for each amount of context N and print it out.\n",
        "import numpy as np\n",
        "\n",
        "q_only = q_counts[1:]\n",
        "\n",
        "col_sums = q_only.sum(axis=0, keepdims=True)\n",
        "q_probs = q_only / col_sums\n",
        "\n",
        "lower_bounds = []\n",
        "for N in range(q_probs.shape[1]):\n",
        "    q = q_probs[:, N]\n",
        "    q_next = np.append(q[1:], 0.0)\n",
        "    terms = np.arange(1, 28) * (q - q_next) * np.log2(np.arange(1, 28))\n",
        "    H_lower = np.sum(terms)\n",
        "    lower_bounds.append(H_lower)\n",
        "\n",
        "for N, H in enumerate(lower_bounds):\n",
        "    print(f\"{H:.4f} bits\")\n"
      ],
      "metadata": {
        "id": "QjRBkxVdB-4S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3b4fcdb-1b27-46e4-f27f-99fbf4f911e6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0303 bits\n",
            "3.0633 bits\n",
            "2.0952 bits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zipf's Law\n",
        "\n",
        "Now let's look at some text data directly to see the skewed distribution of tokens predicted by Zipf's Law. Recall that Zipf's law states that a word's rank (from the most common word at rank 1 on down) to its frequency is approximately a constant, i.e., $r \\cdot f = k$. Equivalently, we can divide both sides by the total number of tokens $N$ to get $r \\cdot P_r = c$, where $c = k/N$ and $P_r = f/N$ is the _relative frequency_ of word $r$.\n",
        "\n",
        "We start by downloading a sample of 1000 open-access English books from [Project Gutenberg](https://gutenberg.org/)."
      ],
      "metadata": {
        "id": "QRingmXrfJSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If your local environment doesn't have the wget command,\n",
        "# you can comment this out and download it manually.\n",
        "!wget \"http://khoury.northeastern.edu/home/dasmith/pg-sample.json.gz\""
      ],
      "metadata": {
        "id": "iwJS1ClihN_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The file is compressed with gzip and is in a JSON lines format. Each line is one JSON record, which we parse with the `json` library.\n",
        "\n",
        "Here we print out the keys in the first record: `id`, `author`, `title`, and `text`."
      ],
      "metadata": {
        "id": "rcOHU_GPiYwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip, json\n",
        "for line in gzip.open(\"pg-sample.json.gz\", mode=\"rt\", encoding=\"utf-8\"):\n",
        "  rec = json.loads(line)\n",
        "  print(rec.keys())\n",
        "  print(rec['author'])\n",
        "  print(rec['title'])\n",
        "  print(rec['text'][0:100])\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L3x1DRZhfTb",
        "outputId": "5afba698-3002-403e-aba1-9a0ca283ccb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['id', 'author', 'title', 'text'])\n",
            "Jefferson, Thomas\n",
            "The Declaration of Independence of the United States of America\n",
            "\n",
            "\n",
            "This is a retranscription of one of the first Project\n",
            "Gutenberg Etexts, offically dated December 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task now is to **tokenize** the text in the `text` field of each record into an array of words. Later on in this course, we will discuss learning better tokenizers. For now, you should separate words on whitespace (space, newline, tab) and punctuation. Convert the tokens to lower case, and keep only those tokens that have at least one letter a-z in them. In general, numerals in text tend not to follow Zipf's law but [Benford's law](https://en.wikipedia.org/wiki/Benford%27s_law).\n",
        "\n",
        "You might use _regular expressions_ (e.g., the `re.split` function) to help with tokenization and filtering.\n",
        "\n",
        "After you have tokenized, compute $N$, the total number of tokens in the corpus and print it out."
      ],
      "metadata": {
        "id": "XbfYV9Fyi2UC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Compute an array of tokens in the corpus\n",
        "# Compute the total number of tokens N and print it out."
      ],
      "metadata": {
        "id": "EsVzek9vkgFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, count the frequency each unigram (distinct word) in the corpus and sort them in an array in descending order of frequency. The first item in your array should be the most common word. Print out that word and its frequency"
      ],
      "metadata": {
        "id": "XkRccWKsk4TE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Compute an arrary of unigrams in descending order of frequency.\n",
        "# Print the most common word and its frequency."
      ],
      "metadata": {
        "id": "jhJ7ec3jk2eS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you can look at the Zipf's law relationship between rank and relative frequency (i.e., frequency divided by $N$). Plot the data using a python graphing package such as matplotlib, plotly, or plotnine. This doesn't have to be a fancy graph, so use whatever you're familiar with. Both axes should be on a log scale. If your package doesn't support log scales, you can take the log of the rank and relative frequency yourself before plotting. Recall that since python arrays are zero-indexed, the rank 1 word will be element 0 of your sorted array."
      ],
      "metadata": {
        "id": "ubHNre1ollvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Plot rank vs. relative frequency of unigrams."
      ],
      "metadata": {
        "id": "H1wq5OGBmJP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, take your array of tokens and compute the counts of both the bigrams and trigrams and sort them in descending order of frequency. Print out the most common bigram and trigram."
      ],
      "metadata": {
        "id": "dTVQFNXOmc34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Compute sorted bigram and trigram statistics.\n",
        "# Print out the most common bigram and trigram.\n",
        "# Plot rank vs. relative frequency for bigrams and trigrams.\n",
        "# You may make separate plots or put them on the same plot and label them."
      ],
      "metadata": {
        "id": "glYoeIsJmm_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**: Finally, write your visual impressions of the fit of the unigram, bigram, and trigram distributions. This doesn't need to be statistically rigorous."
      ],
      "metadata": {
        "id": "TdUTKyE6m0qF"
      }
    }
  ]
}